NAME: Jonathan Chon
EMAIL: jonchon@gmail.com
ID: 104780881

Question 2.3.1 - Cycles in the basic list implementation:
For 1- and 2-thread lists, most of the cycles are spent in doing list operations. Since there are only 1 or 2 threads running, there will not be much time wasted in the critical sections. Thus, list operations is the most expensive part of the code.
In a high-thread spin lock case, most of the cycles are spent wasting the CPU by spinning and waiting for the lock to unlock. With many threads trying to obtain the lock, the lock is the bottleneck in performance.
In a high-thread mutex lock case, most of the time will be spent on system calls. With a mutex lock, when a thread realizes it does not have a lock, it goes to sleep by using a system call. Thus, it does not waste the CPU like spin-lock does, but since system calls are expensive, using these system calls causes the program to slow down.

Question 2.3.2 - Execution Profiling:
The lines of code that are consuming most of the cyles with high-thread spin lock is lines 153 and 235, both of which are in my list_ops function. These lines are both exactly the same: while(__sync_lock_test_and_set(&spin[list_ind[i]], 1);. This is exactly as predicted in question 2.3.1. Since in a spin lock case, a thread without the lock just spins and wastes CPU cyles, with a high number of threads, a thread waiting for the lock is expected to spin for a long amount of time waiting for its turn to obtain the lock.

Question 2.3.3 - Mutex Wait Time:
Lock time rises so dramatically with the number of contending threads since only one thread can enter the critical section at a time. Thus, with many threads, the other threads wake up and sleep again if they do not obtain the lock, which uses a lot of processing power since system calls are expensive. 
Completion time rises less dramatically because completion time also includes the mutex overhead, which is rising dramatically. However, completion time also contains other actions, which essentially "mutes" the mutex overhead's effect to an extent.
It is possible for the wait time per operation to go up faster and higher than the completion time per operation because with high threads, the wait time for mutex lock grows fast. However, the CPU needed for list operations remains consistent, so the wait time for completion will increase gradually, while the wait time for mutex will grow fast as contention for the lock increases.

Question 2.3.4 - Performance of Partitioned Lists:
As the number of lists increases, the throughput also increases. This is because there are more locks available, reducing the amount of time wasted waiting for the lock.
As the number of lists keeps increasing, it will eventually hit a limit. This is because having more lists than threads will not help. When the number of lists equals the number of threads (assuming even distribution of lists to threads), each thread will have their own list to work on. This means that there is no waiting for locks, since each thread will have their own lock. So having more lists than threads will not help.
It does appear reasonable to suggest that the throughput of an N-way partitioned list to be equivalent to the throughput of a single list with (1/N) threads. For example, looking at both mutex and spin-lock protected graphs seem to show that a 4-thread 4-partioned list has approximately the same throughput as a 1-thread 1-partitioned list.

FILES:
lab2_list.c:
Source code for a program that uses the methods defined in SortedList.c and apply them. Contains protection against race conditions in critical sections. Contains the options --threads=, which creates the listed amount of threads, --iterations= which tells how many iterations each thread goes through, --sync= which says which protection should be used for critical sections, --yield= which causes a thread to automatically yield without waiting for a time slice to preempt it, and --list= which splits the main linked lists into the amount of lists stated.

SortedList.h:
Lists the methods implemented in SortedList.c and contains the definitions for SortedList_t and SortedListElement_t

SortedList.c:
Implements the methods listed in SortedList.h. Has different methods to add a node to a circularly linked list, delete a node from a circularly linked list, count the amount of nodes, and lookup the location of a particular node.

lab2_list.gp:
Contains the script to use gnuplot to create plots using lab2b_list.csv.

Makefile:
Contains targets for build, tests, graphs, profile, dist, and clean. Build creates an executable from lab2_list.c, tests calls clean and build and runs listscript.sh. Graphs calls tests and creates the graphs by running lab2_list.gp. Profile calls clean and build and creates profile.out. Dist creates a tarball with the files listed in the specs and listscript.sh. Clean removes the executable created by build and the tarball generated by dist.

Profile.out:
Contains the execution profiling report that shows where time was spent in a spin-lock implementation. Uses ./lab2_list --threads=12 --iterations=1000 --sync=s.

README:
Contains the answers to the questions listed on the specs and includes descriptions of all the files in the generated tarball.

*.png:
Contains the generated plots from the information in lab2b_list.csv

lab2b_list.csv:
Contains the data generated by running the tests listed in the specs, which are contained in liscript.sh

listscript.sh:
Contains the test conditions for lab2_list to generated lab2b_list.csv as specified in the specs.